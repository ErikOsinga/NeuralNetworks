logbook



We are trying a MLP with the following architecture

batch_size = 1
epochs = 20

model = Sequential()
# divide number of inputs by a lot
model.add(Dense(786432//(2048*8), activation='relu', input_shape=(786432,)))
# model.add(Dropout(0.2))
# and then multiplying again
model.add(Dense(786432, activation='sigmoid'))
model.compile(loss='mean_squared_error',
              optimizer=RMSprop(),
              metrics=['accuracy'])


Which is basically a sort of autoencoder?

We accidently put the satellite images as y_train, but the network was already learning to output the gray image with the Google logo at the bottom. 
--> should put this in the report


Should probably try to use binary_crossentropy loss, like in the MNIST example, maybe with optimizer adadelta or adam?


We accidently screwed up the train/test split, but still it seemed to learn the general colors.


Now, running the network again, expecting no large bugs in the code, we change the batch size to 5,

We inspect the images, and it seems the network outputs the same image for every input image.
Maybe this is because of the batch size, where it gets multiple images before learning or its because of the loss function, because this is how it minimizes the error?

Batch size to 1 doesn't fix this issue, we will try a new loss function and optimizer.

With adadelta and binary crossentropy the result is a gray image after 20 epochs, with batch_size = 1. 
Maybe this makes for a slower learning process, so we try an additional 60 epochs after this.
The network doesn't improve beyond loss: 0.3461 - val_loss: 0.3474

We try to use the ADAM optimizer now, but we think it has to do with the loss function.
ADAM shows a totally different image, but with about the same loss function. But the image is still a stack of multiple road maps on top of each other.


So then in conclusion, it seems that ADAM + binary crossentropy outputs about the same image as RMSProp + mean squared error

So what about ADAM + mean squared error?
--> got stuck in a local minimum, outputs 1s everywhere --> white image
--> twice.


So what about RMSProp + binary crossentropy?
Output looks very promising. Saved as batchsize1_epochs_20_RMSProp_BCE
Upon further inspection, it seems like, again the network is only outputting one single image, but now this time it looks a lot like the first image, but not like the other images.
Weirdly, it seems like the network learned the first example??
It's also outputting this example on the test set.
With 40 epochs in total it seems like it is starting to merge the features again.

Will ask tomorrow why the network would only output the same image every time.


We tried adding 0.2 dropout rate after the first layer, but this doesnt seem to help.



We'll try 500 images now with a batch size of 5, maybe less overfitting? Nope, still same problem..


Batch size back to 1, and number of images back to 100, maybe different loss function?

LATER: Maybe try a denoising autoencoder?



For now, try smaller images, and we will add one zoom increment, 
so images are now 100x100x3 and with zoom 16, we will probably just see one or zero roads
Maybe this simple problem is easier to learn? 
The reasoning behind this was, that now our images have 800,000 dimensions which we are trying to learn with only 100 input images, so if we reduce the dimension size we can also reduce the number of input images, and probably the network will learn?


So will try to use 28x28x3 images, and about 10x as much as input size



Seems downloading the images went wrong starting from 6358...
which is coincidentally the number we start at.
But this does seem to get learned by the network.


Will try a new approach to download the images, namely downloading images in a defined square,
which is now bound by the opposite corners, Leiden and Salzburg.


Also will split the original 512x512 images we had downloaded at zoom 15 into smaller
28x28 images, this will probably be faster for downloading data, as it queries the website less.
And it does not have the Google logo on every image!!
So we should stop one row earlier when splitting to get rid of the Google Logo.
Thought: At the same time it could be harder for the network, because some parts of the images
seem very weird, for example there is buildings in the original satellite image 0
and then in the roadmap there is water.
This will affect the cost function a lot in the small images, since water comes out of nowhere in the whole image, but in the large image this is just a small error. I think this will average out though.

Splitting 8900 512x512 images into 324 images makes for 2.8 MILLION IMAGES, totalling only 5.7 GB roundabout.



